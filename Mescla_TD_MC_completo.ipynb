{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38wxDqO_j_IX",
        "outputId": "a4596970-cdb6-4325-b0ba-62f7079c6a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhYe-XZ-ZQup"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Esta função define uma política, em função da tabela Q e do epsilon\n",
        "# Escolhe a ação gulosa (greedy) com probabilidade 1-epsilon e uma ação aleatória com probabilidade epsilon.\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    Q_state = Q[state]\n",
        "    num_actions = len(Q_state)\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        # em caso de empates, retorna sempre o menor índice -- mais eficiente, porém não é bom para alguns ambientes\n",
        "        return np.argmax(Q_state)\n",
        "\n",
        "\n",
        "# Esta função define uma política, em função da tabela Q e do epsilon.\n",
        "# Como a anterior, mas aleatoriza a escolha em caso de haver mais de uma opção gulosa empatada.\n",
        "def epsilon_greedy_random_tiebreak(qtable, state, epsilon):\n",
        "    q_state = qtable[state]\n",
        "    num_actions = len(q_state)\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        # retorna uma ação de valor máximo -- aleatoriza em caso de empates\n",
        "        return np.random.choice(np.where(q_state == q_state.max())[0])\n",
        "\n",
        "\n",
        "def _delete_files(folder, prefix, suffix):\n",
        "    import os\n",
        "    # check if folder exists\n",
        "    if not os.path.exists(folder):\n",
        "        return\n",
        "    # list files and delete all files with the given prefix and the given suffix\n",
        "    for file in os.listdir(folder):\n",
        "        if file.startswith(prefix) and file.endswith(suffix):\n",
        "            os.remove(os.path.join(folder, file))\n",
        "\n",
        "\n",
        "def record_video_qtable(env_name, qtable, episodes=2, folder='videos/', prefix='rl-video', epsilon=0.0, max_episode_length=500):\n",
        "    \"\"\"\n",
        "    Grava um vídeo a partir de uma política epsilon-greedy definida pela 'qtable' e pelo valor de 'epsilon'.\n",
        "    - env_name: A string do ambiente cadastrada no gymnasium ou uma instância da classe. Ao final, o ambiente é fechado (função `close()`).\n",
        "    - qtable: A tabela Q (Q-table) na forma de array bidimensional, com linha representando estados e colunas representando ações.\n",
        "    - length: Número de passos do ambiente usados no vídeo.\n",
        "    - prefiz: Prefixo do nome dos arquivos de vídeo.\n",
        "    - folder: Pasta onde os arquivos de vídeo serão salvos.\n",
        "    - epsilon: Valor do parâmetro da política \"epsilon-greedy\" usada para escolher as ações.\n",
        "    \"\"\"\n",
        "    if isinstance(env_name, str):\n",
        "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = env_name\n",
        "\n",
        "    # delete .mp4 files with the given prefix from the folder\n",
        "    _delete_files(folder, prefix, \".mp4\")\n",
        "\n",
        "    rec_env = gym.wrappers.RecordVideo(env, folder, episode_trigger=lambda i : True, video_length=max_episode_length, name_prefix=prefix)\n",
        "    for _ in range(episodes):\n",
        "        state, _ = rec_env.reset()\n",
        "        ep_steps = 0\n",
        "        done = False\n",
        "        while (not done) and (ep_steps < max_episode_length-1):  # porque o reset conta no tamanho do vídeo\n",
        "            action = epsilon_greedy_random_tiebreak(qtable, state, epsilon)\n",
        "            state, _, termi, trunc, _ = rec_env.step(action)\n",
        "            done = termi or trunc\n",
        "            ep_steps += 1\n",
        "    rec_env.close()\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def evaluate_qtable_policy(env, qtable, num_episodes=100, epsilon=0.0, verbose=False):\n",
        "    \"\"\"\n",
        "    Avalia a política epsilon-greedy definida implicitamente por uma Q-table.\n",
        "    Por padrão, executa com epsilon=0.0; ou seja, executa, em todo estado s, escolhe a ação \"a = argmax Q(s,_)\".\n",
        "    - env: O ambiente instanciado. Ele não é fechado ao final.\n",
        "    - qtable: A Q-table (tabela Q) que será usada.\n",
        "    - num_episodes: Quantidade de episódios a serem executados.\n",
        "    - epsilon: Valor do parâmetro para a escolha epsilon-greedy da ação.\n",
        "\n",
        "    Retorna:\n",
        "    - um par contendo:\n",
        "       -  o valor escalar do retorno médio por episódio\n",
        "       -  e a lista de retornos de todos os episódios\n",
        "    \"\"\"\n",
        "    episode_returns = []\n",
        "    total_steps = 0\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        if verbose:\n",
        "            print(f\"Episódio {i+1}: \", end=\"\")\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_step = 0\n",
        "        episode_returns.append(0.0)\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy_random_tiebreak(qtable, state, epsilon)\n",
        "            state, reward, termi, trunc, _ = env.step(action)\n",
        "            done = termi or trunc\n",
        "            episode_step += 1\n",
        "            total_steps += 1\n",
        "            episode_returns[-1] += reward\n",
        "            if episode_step == 1500:\n",
        "                print(f\"Too long episode, truncating at step {episode_step}.\")\n",
        "                break\n",
        "        if verbose:\n",
        "            print(episode_returns[-1])\n",
        "\n",
        "    mean_return = np.mean(episode_returns)\n",
        "    print(f\"Retorno médio (por episódio): {mean_return:.2f}, episódios: {len(episode_returns)}, total de passos: {total_steps}\")\n",
        "\n",
        "    return mean_return, episode_returns\n",
        "\n",
        "\n",
        "def repeated_exec_qtable_policy(executions, alg_name, qtable, env, num_iterations, epsilon=0.0):\n",
        "    \"\"\"\n",
        "    Executa várias vezes uma política epsilon-greedy definida com a qtable dada.\n",
        "    Internamente usa o `experiments.repeated_exec()`.\n",
        "    \"\"\"\n",
        "\n",
        "    def run_q_greedy(env, num_steps):\n",
        "        state, _ = env.reset()\n",
        "        rewards = []\n",
        "        for i in range(num_steps):\n",
        "            a = epsilon_greedy_random_tiebreak(qtable, state, epsilon)\n",
        "            state, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            rewards.append(r)\n",
        "            if done:\n",
        "                state, _ = env.reset()\n",
        "        return rewards, None\n",
        "\n",
        "    return repeated_exec(executions, alg_name, run_q_greedy, env, num_iterations)\n",
        "\n",
        "\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "def process_returns_linear_interpolation(step_return_list, total_time):\n",
        "    #assert total_time == step_return_list[-1][0], \"The algorithm did not return a final (partial) return at the last time step!\"\n",
        "    partial_returns = [0] * total_time\n",
        "    X = 0\n",
        "    t1 = 0\n",
        "    for i in range(len(step_return_list)):\n",
        "        t2, Y = step_return_list[i]\n",
        "\n",
        "        # if t1+1 > total_time, it wont't enter the loop\n",
        "        # if t2 > total_time, it will calculate up to total_time\n",
        "        for t in range(t1+1, min(t2, total_time) + 1):\n",
        "            partial_returns[t - 1] = X + ( (Y - X) * (t - t1) / (t2 - t1) )\n",
        "            #alt.: partial_returns[t - 1] = ((t2 - t) * X + (t - t1) * Y) / (t2 - t1)\n",
        "\n",
        "        t1 = t2\n",
        "        X = Y\n",
        "\n",
        "    return partial_returns\n",
        "\n",
        "\n",
        "def repeated_exec(executions, alg_name, algorithm, env, num_iterations, *args, **kwargs):\n",
        "    '''\n",
        "    This file runs repeatedly the given algorithm with the given parameters and returns\n",
        "    results compatible with the functions in 'plot'.\n",
        "\n",
        "    Parameters:\n",
        "    - executions: number of times that the 'algorithm' will be run from the start\n",
        "    - alg_name: a string to represent this setting of algorithm (with the given parameters)\n",
        "    - algorithm: must be a function that receives 'env' then and integer (corresponding to the 'num_iterations') then the list of arguments (given by'*args' and/or **kwargs)\n",
        "    - num_iterations: number of steps or episodes\n",
        "    - *args: list of arguments for 'algorithm'\n",
        "    - **kwargs: named arguments for 'algorithm'\n",
        "    - auto_load: to save the results and reload automatically when re-executed with exactly the same parameters (including the number of executions)\n",
        "    '''\n",
        "    # gets a string to identify the environment\n",
        "    if isinstance(env, gym.Env):\n",
        "        env_name = str(env).replace('<', '_').replace('>', '')\n",
        "    else:\n",
        "        env_name = type(env).__name__\n",
        "    auto_load = False\n",
        "    if ('auto_load' in kwargs):\n",
        "        auto_load = kwargs['auto_load']\n",
        "        kwargs.pop('auto_load')\n",
        "    result_file_name = f\"results/{env_name}-{alg_name}-episodes{num_iterations}-execs{executions}.npy\"\n",
        "    if auto_load and os.path.exists(result_file_name):\n",
        "        print(\"Loading results from\", result_file_name)\n",
        "        RESULTS = np.load(result_file_name, allow_pickle=True)\n",
        "        return RESULTS\n",
        "    rewards = np.zeros(shape=(executions, num_iterations))\n",
        "    t = time.time()\n",
        "    print(f\"Executing {alg_name} ({algorithm}):\")\n",
        "    for i in tqdm(range(executions)):\n",
        "        alg_output = algorithm(env, num_iterations, *args, **kwargs)\n",
        "        temp_returns = alg_output[0]\n",
        "        if isinstance(temp_returns[0], tuple):\n",
        "            # when the algorithm outputs a list of pairs (time, return)\n",
        "            rewards[i] = process_returns_linear_interpolation(temp_returns, num_iterations)\n",
        "        else:\n",
        "            # when the algoritm outputs a simple list of returns\n",
        "            rewards[i] = temp_returns\n",
        "    t = time.time() - t\n",
        "    print(f\"  ({executions} executions of {alg_name} finished in {t:.2f} secs)\")\n",
        "    RESULTS = np.array([alg_name, rewards], dtype=object)\n",
        "    directory = os.path.dirname(result_file_name)\n",
        "    if auto_load:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        np.save(result_file_name, RESULTS, allow_pickle=True)\n",
        "    return alg_name, rewards\n",
        "\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "def worker(args):\n",
        "    i, algorithm, env, num_iterations, alg_args, alg_kwargs = args\n",
        "    try:\n",
        "        temp_returns, _ = algorithm(env, num_iterations, *alg_args, **alg_kwargs)\n",
        "        if isinstance(temp_returns[0], tuple):\n",
        "            # when the algorithm outputs a list of pairs (time, return)\n",
        "            return process_returns_linear_interpolation(temp_returns, num_iterations)\n",
        "        else:\n",
        "            # when the algoritm outputs a simple list of returns\n",
        "            return temp_returns\n",
        "    except Exception as e:\n",
        "        print(f\"Error in execution {i} of {algorithm}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def repeated_exec_parallel(executions, num_cpus, alg_name, algorithm, env_factory, num_iterations, args=(), kwargs=dict(), auto_save_load=False):\n",
        "    env = env_factory()\n",
        "    assert isinstance(env, gym.Env)\n",
        "    env_name = str(env).replace('<', '_').replace('>', '')\n",
        "    result_file_name = f\"results/{env_name}-{alg_name}-episodes{num_iterations}-execs{executions}.npy\"\n",
        "    if auto_save_load and os.path.exists(result_file_name):\n",
        "        print(\"Loading results from\", result_file_name)\n",
        "        RESULTS = np.load(result_file_name, allow_pickle=True)\n",
        "        return RESULTS\n",
        "\n",
        "    rewards = None  # expected final shape: (executions, num_iterations)\n",
        "    t = time.time()\n",
        "    print(f\"Executing {alg_name} ({algorithm}) in {num_cpus} cpus:\")\n",
        "\n",
        "    with multiprocessing.Pool(num_cpus) as p:\n",
        "        args_for_worker = [(i, algorithm, env_factory(), num_iterations, args, kwargs) for i in range(executions)]\n",
        "        rewards_list = p.map(worker, args_for_worker)\n",
        "        # catches any excetion raised for invalid rewards list\n",
        "        try:\n",
        "            rewards = np.array(rewards_list)\n",
        "        except:\n",
        "            print(\"ERROR: invalid rewards list returned by the algorithm!\")\n",
        "            print(\"rewards_list =\", rewards_list)\n",
        "            raise\n",
        "\n",
        "    t = time.time() - t\n",
        "    print(f\"  ({executions} executions of {alg_name} finished in {t:.2f} secs)\")\n",
        "\n",
        "    RESULTS = np.array([alg_name, rewards], dtype=object)\n",
        "    directory = os.path.dirname(result_file_name)\n",
        "    if auto_save_load:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        np.save(result_file_name, RESULTS, allow_pickle=True)\n",
        "\n",
        "    return alg_name, rewards\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def smooth(data, window):\n",
        "  data = np.array(data)\n",
        "  n = len(data)\n",
        "  y = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    start = max(0, i-window+1)\n",
        "    y[i] = data[start:(i+1)].mean()\n",
        "  return y\n",
        "\n",
        "#def moving_average(x, w):\n",
        "#    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "\n",
        "# TODO: remover e trocar por plot_single_result\n",
        "def plot_result(returns, ymax_suggested=None, x_log_scale=False, window=None, x_axis='episode', filename=None, cumulative=False):\n",
        "    '''Exibe um gráfico \"episódio/passo x retorno\", fazendo a média a cada `window` retornos, para suavizar.\n",
        "\n",
        "    Parâmetros:\n",
        "    - returns: se return_type=='episode', este parâmetro é uma lista de retornos a cada episódio; se return_type=='step', é uma lista de pares (passo,retorno)\n",
        "    - ymax_suggested (opcional): valor máximo de retorno (eixo y), se tiver um valor máximo conhecido previamente\n",
        "    - x_log_scale: se for True, mostra o eixo x na escala log (para detalhar mais os resultados iniciais)\n",
        "    - window: permite fazer a média dos últimos resultados, para suavizar o gráfico\n",
        "    - return_type: use 'episode' ou 'step' para indicar o que representa o eixo x; também afeta como será lido o parâmetro 'returns'\n",
        "    - filename: se for fornecida uma string, salva um arquivo de imagem ao invés de exibir.\n",
        "    '''\n",
        "    plt.figure(figsize=(12,7))\n",
        "\n",
        "    # TODO: uniformizar com a outra função\n",
        "    if cumulative == 'no':\n",
        "        cumulative = False\n",
        "\n",
        "    if x_axis == 'episode':\n",
        "        plt.xlabel('Episódios')\n",
        "        if cumulative:\n",
        "            returns = np.array(returns)\n",
        "            returns = np.cumsum(returns)\n",
        "            title = \"Retorno acumulado\"\n",
        "            if window is not None:\n",
        "                print(\"Attention: 'window' is ignored when 'cumulative'==True\")\n",
        "            window = 1\n",
        "        else:\n",
        "            if window is None:\n",
        "                window = 10\n",
        "            title = f\"Retorno médio a cada {window} episódios\"\n",
        "        yvalues = smooth(returns, window)\n",
        "        xvalues = np.arange(1, len(returns)+1)\n",
        "        plt.plot(xvalues, yvalues)\n",
        "        #plt.title(f\"Retorno médio a cada {window} episódios\")\n",
        "        plt.title(title)\n",
        "    #elif x_axis == 'step':\n",
        "    else:\n",
        "        print(\"Attention: 'window' is ignored for 'step' type of returns\")\n",
        "        plt.xlabel('Passos')\n",
        "        xvalues, yvalues = list(zip(*returns))\n",
        "        xvalues = np.array(xvalues) + 1\n",
        "        if cumulative:\n",
        "            yvalues = np.array(yvalues)\n",
        "            yvalues = np.cumsum(yvalues)\n",
        "            title = \"Retorno acumulado\"\n",
        "            # window = 1\n",
        "        else:\n",
        "            #if window is None:\n",
        "            #    window = 10\n",
        "            title = \"Retorno\"\n",
        "        #yvalues = smooth(returns, window)\n",
        "        plt.plot(xvalues, yvalues)\n",
        "        #plt.title(f\"Retorno médio a cada {window} passos\")\n",
        "        plt.title(title)\n",
        "\n",
        "    if x_log_scale:\n",
        "        plt.xscale('log')\n",
        "\n",
        "    plt.ylabel('Retorno')\n",
        "    if ymax_suggested is not None:\n",
        "        ymax = np.max([ymax_suggested, np.max(yvalues)])\n",
        "        plt.ylim(top=ymax)\n",
        "\n",
        "    if filename is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(filename)\n",
        "        print(\"Arquivo salvo:\", filename)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_multiple_results(list_returns, cumulative='no', x_log_scale=False, x_axis='episode', window=10, plot_stddev=False, yreference=None, y_min=None):\n",
        "    '''Exibe um gráfico \"episódio/passo x retorno\" com vários resultados.\n",
        "\n",
        "    Parâmetros:\n",
        "    - list_returns: uma lista de pares (nome do resultado, retorno por episódio/passo)\n",
        "    - cumulative: indica se as recompensas anteriores devem ser acumuladas, para calcular a soma ou média histórica por episódio\n",
        "    - x_log_scale: se for True, mostra o eixo x na escala log (para detalhar mais os resultados iniciais)\n",
        "    - x_axis: use 'episode' ou 'step' para indicar o que representa o eixo x\n",
        "    - window: permite fazer a média dos últimos resultados, para suavizar o gráfico; só é usado se cumulative='no'\n",
        "    - plot_stddev: exibe sombra com o desvio padrão, ou seja, entre média-desvio e média+desvio\n",
        "    - yreference: if not None, should be an integer, where will be plot a horizontal gray dashed line, used for reference\n",
        "    - y_min: valor mínimo do eixo y; caso os dados tenham valor menor, o gráfico será ajustado para adotar este valor como mínimo\n",
        "    '''\n",
        "    # True and False are here for backward compatibility (remove!)\n",
        "    if cumulative is None:\n",
        "        cumulative = 'no'\n",
        "    assert cumulative in ['no', 'sum', 'avg']\n",
        "    assert x_axis in ['step', 'episode']\n",
        "\n",
        "    total_steps = list_returns[0][1].shape[1]\n",
        "    plt.figure(figsize=(12,7))\n",
        "\n",
        "    for (alg_name, returns) in list_returns:\n",
        "        xvalues = np.arange(1, total_steps+1)\n",
        "        # TODO: bug -- isso está errado para cumulative='avg', quando x_axis='step'\n",
        "        if cumulative == 'sum' or cumulative == 'avg':\n",
        "            # calculate the cumulative sum along axis 1\n",
        "            cumreturns = np.cumsum(returns, axis=1)\n",
        "            if cumulative == 'avg':\n",
        "                cumreturns = cumreturns / xvalues\n",
        "            yvalues = cumreturns.mean(axis=0)\n",
        "            std = cumreturns.std(axis=0)\n",
        "        else:\n",
        "            yvalues = smooth(returns.mean(axis=0),window)\n",
        "            std = returns.std(axis=0)\n",
        "        plt.plot(xvalues, yvalues, label=alg_name)\n",
        "        if plot_stddev:\n",
        "            plt.fill_between(xvalues, yvalues-std, yvalues+std, alpha=0.4)\n",
        "\n",
        "    if yreference is not None:\n",
        "        y_ref_line = [ yreference ] * total_steps\n",
        "        plt.plot(y_ref_line, linestyle=\"--\", color=\"gray\")\n",
        "\n",
        "    if x_log_scale:\n",
        "        plt.xscale('log')\n",
        "\n",
        "    if x_axis == 'episode':\n",
        "        plt.xlabel('Episódio')\n",
        "        payoff = 'Retorno'\n",
        "    else:\n",
        "        plt.xlabel('Passo')\n",
        "        payoff = 'Recompensa'\n",
        "\n",
        "    plt.ylabel('Retorno')\n",
        "\n",
        "    if cumulative == 'no':\n",
        "        plt.title(f\"{payoff} (média móvel a cada {window})\")\n",
        "    elif cumulative == 'avg':\n",
        "        gen = payoff[-1]\n",
        "        plt.title(f\"{payoff} acumulad{gen} médi{gen}\")\n",
        "    else:\n",
        "        gen = payoff[-1]\n",
        "        plt.title(f\"{payoff} acumulad{gen}\")\n",
        "\n",
        "    if y_min is not None:\n",
        "        min_value = min(np.min(returns) for (_, returns) in list_returns)\n",
        "        if min_value < y_min:\n",
        "            plt.ylim(bottom=y_min)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# TODO: corrigir bug no caso cumulative='avg' e x_axis='step'\n",
        "def plot_single_result(returns, *args, **kwargs):\n",
        "    '''Exibe um gráfico \"episódio/passo x retorno\", para um único resultado.\n",
        "\n",
        "    Parâmetros:\n",
        "    - cumulative: indica se as recompensas anteriores devem ser acumuladas, para calcular a soma ou média histórica por episódio\n",
        "    - x_log_scale: se for True, mostra o eixo x na escala log (para detalhar mais os resultados iniciais)\n",
        "    - window: permite fazer a média dos últimos resultados, para suavizar o gráfico; só é usado se cumulative='no'\n",
        "    - plot_stddev: exibe sombra com o desvio padrão, ou seja, entre média-desvio e média+desvio\n",
        "    - yreference: if not None, should be an integer, where will be plot a horizontal gray dashed line, used for reference\n",
        "    '''\n",
        "    if isinstance(returns[0], tuple):\n",
        "        # when the algorithm outputs a list of pairs (time, return)\n",
        "        x_axis = 'step'\n",
        "        total_time = returns[-1][0]\n",
        "        returns = process_returns_linear_interpolation(returns, total_time)\n",
        "    else:\n",
        "        # when the algoritm outputs a simple list of returns\n",
        "        x_axis = 'episode'\n",
        "\n",
        "    processed_returns = np.array([returns])\n",
        "    plot_multiple_results([(None, processed_returns)], x_axis=x_axis, *args, **kwargs)\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Esta função define uma política, em função da tabela Q e do epsilon\n",
        "# Escolhe a ação gulosa (greedy) com probabilidade 1-epsilon e uma ação aleatória com probabilidade epsilon.\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    Q_state = Q[state]\n",
        "    num_actions = len(Q_state)\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        # em caso de empates, retorna sempre o menor índice -- mais eficiente, porém não é bom para alguns ambientes\n",
        "        return np.argmax(Q_state)\n",
        "\n",
        "\n",
        "# Esta função define uma política, em função da tabela Q e do epsilon.\n",
        "# Como a anterior, mas aleatoriza a escolha em caso de haver mais de uma opção gulosa empatada.\n",
        "def epsilon_greedy_random_tiebreak(qtable, state, epsilon):\n",
        "    q_state = qtable[state]\n",
        "    num_actions = len(q_state)\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        # retorna uma ação de valor máximo -- aleatoriza em caso de empates\n",
        "        return np.random.choice(np.where(q_state == q_state.max())[0])\n",
        "\n",
        "\n",
        "def _delete_files(folder, prefix, suffix):\n",
        "    import os\n",
        "    # check if folder exists\n",
        "    if not os.path.exists(folder):\n",
        "        return\n",
        "    # list files and delete all files with the given prefix and the given suffix\n",
        "    for file in os.listdir(folder):\n",
        "        if file.startswith(prefix) and file.endswith(suffix):\n",
        "            os.remove(os.path.join(folder, file))\n",
        "\n",
        "\n",
        "def record_video_qtable(env_name, qtable, episodes=2, folder='videos/', prefix='rl-video', epsilon=0.0, max_episode_length=500):\n",
        "    \"\"\"\n",
        "    Grava um vídeo a partir de uma política epsilon-greedy definida pela 'qtable' e pelo valor de 'epsilon'.\n",
        "    - env_name: A string do ambiente cadastrada no gymnasium ou uma instância da classe. Ao final, o ambiente é fechado (função `close()`).\n",
        "    - qtable: A tabela Q (Q-table) na forma de array bidimensional, com linha representando estados e colunas representando ações.\n",
        "    - length: Número de passos do ambiente usados no vídeo.\n",
        "    - prefiz: Prefixo do nome dos arquivos de vídeo.\n",
        "    - folder: Pasta onde os arquivos de vídeo serão salvos.\n",
        "    - epsilon: Valor do parâmetro da política \"epsilon-greedy\" usada para escolher as ações.\n",
        "    \"\"\"\n",
        "    if isinstance(env_name, str):\n",
        "        env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    else:\n",
        "        env = env_name\n",
        "\n",
        "    # delete .mp4 files with the given prefix from the folder\n",
        "    _delete_files(folder, prefix, \".mp4\")\n",
        "\n",
        "    rec_env = gym.wrappers.RecordVideo(env, folder, episode_trigger=lambda i : True, video_length=max_episode_length, name_prefix=prefix)\n",
        "    for _ in range(episodes):\n",
        "        state, _ = rec_env.reset()\n",
        "        ep_steps = 0\n",
        "        done = False\n",
        "        while (not done) and (ep_steps < max_episode_length-1):  # porque o reset conta no tamanho do vídeo\n",
        "            action = epsilon_greedy_random_tiebreak(qtable, state, epsilon)\n",
        "            state, _, termi, trunc, _ = rec_env.step(action)\n",
        "            done = termi or trunc\n",
        "            ep_steps += 1\n",
        "    rec_env.close()\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def evaluate_qtable_policy(env, qtable, num_episodes=100, epsilon=0.0, verbose=False):\n",
        "    \"\"\"\n",
        "    Avalia a política epsilon-greedy definida implicitamente por uma Q-table.\n",
        "    Por padrão, executa com epsilon=0.0; ou seja, executa, em todo estado s, escolhe a ação \"a = argmax Q(s,_)\".\n",
        "    - env: O ambiente instanciado. Ele não é fechado ao final.\n",
        "    - qtable: A Q-table (tabela Q) que será usada.\n",
        "    - num_episodes: Quantidade de episódios a serem executados.\n",
        "    - epsilon: Valor do parâmetro para a escolha epsilon-greedy da ação.\n",
        "\n",
        "    Retorna:\n",
        "    - um par contendo:\n",
        "       -  o valor escalar do retorno médio por episódio\n",
        "       -  e a lista de retornos de todos os episódios\n",
        "    \"\"\"\n",
        "    episode_returns = []\n",
        "    total_steps = 0\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        if verbose:\n",
        "            print(f\"Episódio {i+1}: \", end=\"\")\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_step = 0\n",
        "        episode_returns.append(0.0)\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy_random_tiebreak(qtable, state, epsilon)\n",
        "            state, reward, termi, trunc, _ = env.step(action)\n",
        "            done = termi or trunc\n",
        "            episode_step += 1\n",
        "            total_steps += 1\n",
        "            episode_returns[-1] += reward\n",
        "            if episode_step == 1500:\n",
        "                print(f\"Too long episode, truncating at step {episode_step}.\")\n",
        "                break\n",
        "        if verbose:\n",
        "            print(episode_returns[-1])\n",
        "\n",
        "    mean_return = np.mean(episode_returns)\n",
        "    print(f\"Retorno médio (por episódio): {mean_return:.2f}, episódios: {len(episode_returns)}, total de passos: {total_steps}\")\n",
        "\n",
        "    return mean_return, episode_returns\n",
        "\n",
        "\n",
        "def repeated_exec_qtable_policy(executions, alg_name, qtable, env, num_iterations, epsilon=0.0):\n",
        "    \"\"\"\n",
        "    Executa várias vezes uma política epsilon-greedy definida com a qtable dada.\n",
        "    Internamente usa o `experiments.repeated_exec()`.\n",
        "    \"\"\"\n",
        "    from experiments import repeated_exec\n",
        "\n",
        "    def run_q_greedy(env, num_steps):\n",
        "        state, _ = env.reset()\n",
        "        rewards = []\n",
        "        for i in range(num_steps):\n",
        "            a = epsilon_greedy_random_tiebreak(qtable, state, epsilon)\n",
        "            state, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            rewards.append(r)\n",
        "            if done:\n",
        "                state, _ = env.reset()\n",
        "        return rewards, None\n",
        "\n",
        "    return repeated_exec(executions, alg_name, run_q_greedy, env, num_iterations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Importações\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import optuna"
      ],
      "metadata": {
        "id": "FCDcQf8v_4-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Funções Auxiliares e Estratégias de Seleção de Ação\n",
        "\n",
        "def plot_learning_curve(ax, rewards, window=50, label=None):\n",
        "    \"\"\"Plota a média móvel dos retornos no eixo 'ax'.\"\"\"\n",
        "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "    ax.plot(moving_avg, label=label)\n",
        "    ax.set_xlabel(\"Episódios\")\n",
        "    ax.set_ylabel(\"Retorno médio\")\n",
        "\n",
        "def softmax_probs(Q_state):\n",
        "    values = Q_state - np.max(Q_state)\n",
        "    exp_values = np.exp(values)\n",
        "    return exp_values / np.sum(exp_values)\n",
        "\n",
        "def softmax_choice(Q, state):\n",
        "    probs = softmax_probs(Q[state])\n",
        "    return np.random.choice(len(probs), p=probs)\n",
        "\n",
        "def epsilon_greedy_probs(Q, state, epsilon):\n",
        "    Q_state = Q[state]\n",
        "    num_actions = len(Q_state)\n",
        "    q_max = np.max(Q_state)\n",
        "    non_greedy_prob = epsilon / num_actions\n",
        "    num_greedy = np.sum(Q_state == q_max)\n",
        "    greedy_prob = ((1 - epsilon) / num_greedy) + non_greedy_prob\n",
        "    probs = np.where(Q_state == q_max, greedy_prob, non_greedy_prob)\n",
        "    return probs\n",
        "\n",
        "def epsilon_greedy_choice(Q, state, epsilon=0.1):\n",
        "    num_actions = len(Q[state])\n",
        "    probs = epsilon_greedy_probs(Q, state, epsilon)\n",
        "    return np.random.choice(num_actions, p=probs)\n"
      ],
      "metadata": {
        "id": "viGHdVW-_7AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Definição dos Algoritmos\n",
        "\n",
        "def run_expected_sarsa(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1):\n",
        "    \"\"\"Algoritmo Expected-SARSA\"\"\"\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.random.uniform(low=-0.01, high=0.01, size=(env.observation_space.n, num_actions))\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy_choice(Q, state, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if terminated:\n",
        "                V_next = 0\n",
        "            else:\n",
        "                p_next = epsilon_greedy_probs(Q, next_state, epsilon)\n",
        "                V_next = np.sum(p_next * Q[next_state])\n",
        "\n",
        "            Q[state, action] += lr * ((reward + gamma * V_next) - Q[state, action])\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "    return episode_rewards, Q\n",
        "\n",
        "def run_montecarlo2(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1, render_env=None):\n",
        "    \"\"\"Algoritmo Monte Carlo (toda-visita)\"\"\"\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    episode_rewards = []\n",
        "\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        trajectory = []\n",
        "\n",
        "        train_env = render_env if (render_env is not None and ((i+1) % 1000 == 0 or i >= episodes-5)) else env\n",
        "\n",
        "        state, _ = train_env.reset()\n",
        "        while not done:\n",
        "            if np.random.random() < epsilon:\n",
        "                action = np.random.randint(0, num_actions)\n",
        "            else:\n",
        "                action = np.argmax(Q[state])\n",
        "            next_state, reward, terminated, truncated, _ = train_env.step(action)\n",
        "            done = terminated or truncated\n",
        "            trajectory.append((state, action, reward))\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "        G = 0\n",
        "        for (s, a, r) in reversed(trajectory):\n",
        "            G = r + gamma * G\n",
        "            Q[s, a] += lr * (G - Q[s, a])\n",
        "    return episode_rewards, Q\n",
        "\n",
        "def run_hybrid_expected_sarsa_mc(env, episodes, lr=0.1, gamma=0.95, epsilon=0.1):\n",
        "\n",
        "    # Verifica se os espaços do ambiente são discretos (necessário para indexar a Q-table)\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    # Número de ações disponíveis no ambiente\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Inicializa a Q-table com valores pequenos aleatórios para evitar empates\n",
        "    Q = np.random.uniform(low=-0.01, high=+0.01, size=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # Lista para armazenar o retorno (soma de recompensas) de cada episódio\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    # Loop principal de treinamento: itera sobre cada episódio\n",
        "    for i in range(episodes):\n",
        "        done = False               # Flag para controlar quando o episódio termina\n",
        "        sum_rewards = 0            # Acumulador para a recompensa total do episódio\n",
        "        episode_trajectory = []    # Armazena a sequência de transições (estado, ação, recompensa)\n",
        "\n",
        "        # Reinicia o ambiente e obtém o estado inicial\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # -------------------------------\n",
        "        # Parte Online: Atualização Expected-SARSA\n",
        "        # -------------------------------\n",
        "        while not done:\n",
        "            # Seleciona uma ação utilizando a política epsilon-greedy\n",
        "            action = epsilon_greedy_choice(Q, state, epsilon)\n",
        "\n",
        "            # Executa a ação no ambiente e obtém o próximo estado, recompensa e sinal de término\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            # O episódio termina se houver 'terminated' ou 'truncated'\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Armazena a transição para uso na atualização offline\n",
        "            episode_trajectory.append((state, action, reward))\n",
        "\n",
        "            # Se o episódio terminou, o valor do próximo estado é considerado 0\n",
        "            if terminated:\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # Calcula as probabilidades das ações no próximo estado (epsilon-greedy)\n",
        "                p_next_actions = epsilon_greedy_probs(Q, next_state, epsilon)\n",
        "                # Valor esperado para o próximo estado é a soma ponderada dos Q-values\n",
        "                V_next_state = np.sum(p_next_actions * Q[next_state])\n",
        "\n",
        "            # Calcula o erro temporal (TD error) para a atualização online\n",
        "            delta = (reward + gamma * V_next_state) - Q[state, action]\n",
        "            # Atualiza o Q-value para a ação tomada no estado atual\n",
        "            Q[state, action] += lr * delta\n",
        "\n",
        "            # Acumula a recompensa obtida neste passo\n",
        "            sum_rewards += reward\n",
        "            # Atualiza o estado atual para o próximo estado\n",
        "            state = next_state\n",
        "\n",
        "        # Fim do episódio: armazena o total de recompensas obtidas\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # -------------------------------\n",
        "        # Parte Offline: Atualização Monte Carlo\n",
        "        # -------------------------------\n",
        "        # Reinicia o retorno acumulado para o episódio (Gt)\n",
        "        Gt = 0\n",
        "        # Percorre a trajetória do episódio em ordem reversa (do final para o início)\n",
        "        for (s, a, r) in reversed(episode_trajectory):\n",
        "            # Calcula o retorno descontado (G_t = r + gamma * G_{t+1})\n",
        "            Gt = r + gamma * Gt\n",
        "            # Calcula a diferença entre o retorno real e a estimativa atual\n",
        "            delta = Gt - Q[s, a]\n",
        "            # Atualiza o Q-value usando o learning rate\n",
        "            Q[s, a] += lr * delta\n",
        "\n",
        "        # Opcional: a cada 100 episódios, imprime a média dos retornos dos últimos 100 episódios\n",
        "        if (i + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "            print(f\"[Híbrido] Episódio {i+1} - Média últimos 100: {avg_reward:.3f}\")\n",
        "\n",
        "    # Retorna a lista de retornos por episódio e a Q-table final\n",
        "    return all_episode_rewards, Q\n"
      ],
      "metadata": {
        "id": "Rsr_A_j5Ft4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Otimização dos Hiperparâmetros via Optuna para Cada Algoritmo\n",
        "\n",
        "def optimize_expected_sarsa_params(env_name, episodes=200):\n",
        "    \"\"\"Otimiza os hiperparâmetros do Expected-SARSA para um ambiente dado.\"\"\"\n",
        "    def objective(trial):\n",
        "        epsilon = trial.suggest_uniform(\"epsilon\", 0.01, 0.1)\n",
        "        lr_opt = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
        "        gamma = trial.suggest_uniform(\"gamma\", 0.8, 1.0)\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            env = gym.make(env_name, max_episode_steps=500)\n",
        "        else:\n",
        "            env = gym.make(env_name)\n",
        "        rewards, _ = run_expected_sarsa(env, episodes, lr=lr_opt, gamma=gamma, epsilon=epsilon)\n",
        "        env.close()\n",
        "        return np.mean(rewards[-100:])\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print(f\"Optuna: Melhores parâmetros para Expected-SARSA em {env_name}: {study.best_params}\")\n",
        "    return study.best_params\n",
        "\n",
        "def optimize_montecarlo_params(env_name, episodes=200):\n",
        "    \"\"\"Otimiza os hiperparâmetros do Monte Carlo para um ambiente dado.\"\"\"\n",
        "    def objective(trial):\n",
        "        epsilon = trial.suggest_uniform(\"epsilon\", 0.01, 0.1)\n",
        "        lr_opt = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
        "        gamma = trial.suggest_uniform(\"gamma\", 0.8, 1.0)\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            env = gym.make(env_name, max_episode_steps=500)\n",
        "        else:\n",
        "            env = gym.make(env_name)\n",
        "        rewards, _ = run_montecarlo2(env, episodes, lr=lr_opt, gamma=gamma, epsilon=epsilon)\n",
        "        env.close()\n",
        "        return np.mean(rewards[-100:])\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print(f\"Optuna: Melhores parâmetros para Monte Carlo em {env_name}: {study.best_params}\")\n",
        "    return study.best_params\n",
        "\n",
        "def optimize_hybrid_params(env_name, episodes=200):\n",
        "    \"\"\"Otimiza os hiperparâmetros do algoritmo Híbrido para um ambiente dado.\"\"\"\n",
        "    def objective(trial):\n",
        "        epsilon = trial.suggest_uniform(\"epsilon\", 0.01, 0.1)\n",
        "        lr_opt = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
        "        gamma = trial.suggest_uniform(\"gamma\", 0.8, 1.0)\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            env = gym.make(env_name, max_episode_steps=500)\n",
        "        else:\n",
        "            env = gym.make(env_name)\n",
        "        rewards, _ = run_hybrid_expected_sarsa_mc(env, episodes, lr=lr_opt, gamma=gamma, epsilon=epsilon)\n",
        "        env.close()\n",
        "        return np.mean(rewards[-100:])\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10)\n",
        "    print(f\"Optuna: Melhores parâmetros para Híbrido em {env_name}: {study.best_params}\")\n",
        "    return study.best_params\n"
      ],
      "metadata": {
        "id": "xsw95TMyFuSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Parâmetros dos Experimentos\n",
        "\n",
        "# Lista de learning rates para variar (usados em Expected-SARSA e Monte Carlo)\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
        "\n",
        "# Definindo os ambientes e o número de episódios para cada um\n",
        "env_configs = {\n",
        "    \"Taxi-v3\": {\"episodes\": 5000},\n",
        "    \"FrozenLake-v1\": {\"episodes\": 5000},\n",
        "    \"CliffWalking-v0\": {\"episodes\": 5000}\n",
        "}\n"
      ],
      "metadata": {
        "id": "WFGJOpvXABP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Loop de Experimentos, Otimização e Plotagem\n",
        "\n",
        "# Para cada ambiente, otimiza os hiperparâmetros de cada algoritmo via Optuna,\n",
        "# executa o experimento completo com os melhores parâmetros e plota as curvas de aprendizado.\n",
        "for env_name, config in env_configs.items():\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Expected-SARSA\n",
        "    print(f\"\\nOtimização do Expected-SARSA para {env_name}...\")\n",
        "    best_params_ES = optimize_expected_sarsa_params(env_name, episodes=200)\n",
        "    if env_name == \"CliffWalking-v0\":\n",
        "        env = gym.make(env_name, max_episode_steps=500)\n",
        "    else:\n",
        "        env = gym.make(env_name)\n",
        "    print(f\"Executando Expected-SARSA em {env_name} com parâmetros: {best_params_ES}\")\n",
        "    rewards_ES, _ = run_expected_sarsa(env, episodes=config[\"episodes\"],\n",
        "                                       lr=best_params_ES[\"lr\"],\n",
        "                                       gamma=best_params_ES[\"gamma\"],\n",
        "                                       epsilon=best_params_ES[\"epsilon\"])\n",
        "    env.close()\n",
        "    plot_learning_curve(ax, rewards_ES, window=50, label=\"Expected-SARSA\")\n",
        "\n",
        "    # Monte Carlo\n",
        "    print(f\"\\nOtimização do Monte Carlo para {env_name}...\")\n",
        "    best_params_MC = optimize_montecarlo_params(env_name, episodes=200)\n",
        "    if env_name == \"CliffWalking-v0\":\n",
        "        env = gym.make(env_name, max_episode_steps=500)\n",
        "    else:\n",
        "        env = gym.make(env_name)\n",
        "    print(f\"Executando Monte Carlo em {env_name} com parâmetros: {best_params_MC}\")\n",
        "    rewards_MC, _ = run_montecarlo2(env, episodes=config[\"episodes\"],\n",
        "                                    lr=best_params_MC[\"lr\"],\n",
        "                                    gamma=best_params_MC[\"gamma\"],\n",
        "                                    epsilon=best_params_MC[\"epsilon\"])\n",
        "    env.close()\n",
        "    plot_learning_curve(ax, rewards_MC, window=50, label=\"Monte Carlo\")\n",
        "\n",
        "    # Híbrido\n",
        "    print(f\"\\nOtimização do Híbrido para {env_name}...\")\n",
        "    best_params_HYB = optimize_hybrid_params(env_name, episodes=200)\n",
        "    if env_name == \"CliffWalking-v0\":\n",
        "        env = gym.make(env_name, max_episode_steps=500)\n",
        "    else:\n",
        "        env = gym.make(env_name)\n",
        "    print(f\"Executando Híbrido em {env_name} com parâmetros: {best_params_HYB}\")\n",
        "    rewards_HYB, _ = run_hybrid_expected_sarsa_mc(env, episodes=config[\"episodes\"],\n",
        "                                                  lr=best_params_HYB[\"lr\"],\n",
        "                                                  gamma=best_params_HYB[\"gamma\"],\n",
        "                                                  epsilon=best_params_HYB[\"epsilon\"])\n",
        "    env.close()\n",
        "    plot_learning_curve(ax, rewards_HYB, window=50, label=\"Híbrido\")\n",
        "\n",
        "    ax.set_title(f\"{env_name} - Comparação dos Algoritmos (com Optuna)\")\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR5JpCTmAD3B",
        "outputId": "08621465-bc59-441c-c44b-3ee899bad12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:48:43,489] A new study created in memory with name: no-name-5c504672-9271-4b61-804b-1ed2539719f2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Otimização do Expected-SARSA para Taxi-v3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-cd7b9cca50e1>:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  epsilon = trial.suggest_uniform(\"epsilon\", 0.01, 0.1)\n",
            "<ipython-input-54-cd7b9cca50e1>:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr_opt = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-54-cd7b9cca50e1>:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.8, 1.0)\n",
            "[I 2025-02-21 01:48:54,091] Trial 0 finished with value: -286.9 and parameters: {'epsilon': 0.04713339470847364, 'lr': 0.0001576871491840037, 'gamma': 0.8239351943205514}. Best is trial 0 with value: -286.9.\n",
            "[I 2025-02-21 01:49:01,115] Trial 1 finished with value: -261.35 and parameters: {'epsilon': 0.07422009310191259, 'lr': 0.01085996432270055, 'gamma': 0.822968385381648}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:06,477] Trial 2 finished with value: -268.54 and parameters: {'epsilon': 0.0906158218430787, 'lr': 0.008554563190940525, 'gamma': 0.8568565486521351}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:11,427] Trial 3 finished with value: -270.14 and parameters: {'epsilon': 0.03372971788073343, 'lr': 0.00032449956765381535, 'gamma': 0.9626863402435875}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:16,949] Trial 4 finished with value: -269.93 and parameters: {'epsilon': 0.060791442488086586, 'lr': 0.002073584018208823, 'gamma': 0.8281760147197004}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:22,270] Trial 5 finished with value: -304.22 and parameters: {'epsilon': 0.08801471830250572, 'lr': 0.00012362448872149588, 'gamma': 0.9670896658790937}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:27,242] Trial 6 finished with value: -281.02 and parameters: {'epsilon': 0.0838772956643562, 'lr': 0.0008672092100420381, 'gamma': 0.8393201141489682}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:32,845] Trial 7 finished with value: -285.89 and parameters: {'epsilon': 0.021572968268846512, 'lr': 0.00033986321058732626, 'gamma': 0.9740311951833833}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:37,648] Trial 8 finished with value: -280.7 and parameters: {'epsilon': 0.09905006426535275, 'lr': 0.004465624060236887, 'gamma': 0.8048252454476873}. Best is trial 1 with value: -261.35.\n",
            "[I 2025-02-21 01:49:43,148] Trial 9 finished with value: -304.04 and parameters: {'epsilon': 0.08895455395790992, 'lr': 9.089221391031215e-05, 'gamma': 0.9220302358044599}. Best is trial 1 with value: -261.35.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna: Melhores parâmetros para Expected-SARSA em Taxi-v3: {'epsilon': 0.07422009310191259, 'lr': 0.01085996432270055, 'gamma': 0.822968385381648}\n",
            "Executando Expected-SARSA em Taxi-v3 com parâmetros: {'epsilon': 0.07422009310191259, 'lr': 0.01085996432270055, 'gamma': 0.822968385381648}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:51:09,752] A new study created in memory with name: no-name-41e8ae71-4009-48ca-813d-60aaafa20200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Otimização do Monte Carlo para Taxi-v3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-cd7b9cca50e1>:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  epsilon = trial.suggest_uniform(\"epsilon\", 0.01, 0.1)\n",
            "<ipython-input-54-cd7b9cca50e1>:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr_opt = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-54-cd7b9cca50e1>:27: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.8, 1.0)\n",
            "[I 2025-02-21 01:51:10,925] Trial 0 finished with value: -857.36 and parameters: {'epsilon': 0.035314743110489655, 'lr': 2.286395930349363e-05, 'gamma': 0.8792573074286198}. Best is trial 0 with value: -857.36.\n",
            "[I 2025-02-21 01:51:12,112] Trial 1 finished with value: -905.23 and parameters: {'epsilon': 0.07606286992670788, 'lr': 0.010868693233837979, 'gamma': 0.8295971836044354}. Best is trial 0 with value: -857.36.\n",
            "[I 2025-02-21 01:51:13,057] Trial 2 finished with value: -784.73 and parameters: {'epsilon': 0.02698956069614663, 'lr': 0.0011811162059817304, 'gamma': 0.9825395523421797}. Best is trial 2 with value: -784.73.\n",
            "[I 2025-02-21 01:51:14,003] Trial 3 finished with value: -737.3 and parameters: {'epsilon': 0.024507936348339768, 'lr': 1.1197944455955811e-05, 'gamma': 0.9829857220482494}. Best is trial 3 with value: -737.3.\n",
            "[I 2025-02-21 01:51:14,920] Trial 4 finished with value: -695.92 and parameters: {'epsilon': 0.04287820098317527, 'lr': 0.008394871835014065, 'gamma': 0.8646067525922931}. Best is trial 4 with value: -695.92.\n",
            "[I 2025-02-21 01:51:15,832] Trial 5 finished with value: -1014.05 and parameters: {'epsilon': 0.04724624825893981, 'lr': 8.831569932779446e-05, 'gamma': 0.9753626148295793}. Best is trial 4 with value: -695.92.\n",
            "[I 2025-02-21 01:51:16,652] Trial 6 finished with value: -577.91 and parameters: {'epsilon': 0.02309892013391228, 'lr': 0.0012608272268817343, 'gamma': 0.942509250543687}. Best is trial 6 with value: -577.91.\n",
            "[I 2025-02-21 01:51:17,507] Trial 7 finished with value: -728.12 and parameters: {'epsilon': 0.020976255864931816, 'lr': 6.321764312887912e-05, 'gamma': 0.824320683845746}. Best is trial 6 with value: -577.91.\n",
            "[I 2025-02-21 01:51:18,286] Trial 8 finished with value: -663.32 and parameters: {'epsilon': 0.022933597819741896, 'lr': 0.0963400300944861, 'gamma': 0.9642973476124082}. Best is trial 6 with value: -577.91.\n",
            "[I 2025-02-21 01:51:19,293] Trial 9 finished with value: -1005.23 and parameters: {'epsilon': 0.08839174134146792, 'lr': 0.00019716351185877117, 'gamma': 0.9891586223299281}. Best is trial 6 with value: -577.91.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optuna: Melhores parâmetros para Monte Carlo em Taxi-v3: {'epsilon': 0.02309892013391228, 'lr': 0.0012608272268817343, 'gamma': 0.942509250543687}\n",
            "Executando Monte Carlo em Taxi-v3 com parâmetros: {'epsilon': 0.02309892013391228, 'lr': 0.0012608272268817343, 'gamma': 0.942509250543687}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:51:40,267] A new study created in memory with name: no-name-2b3d9e0c-60a9-413b-909d-98954c7bf445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Otimização do Híbrido para Taxi-v3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-54-cd7b9cca50e1>:44: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  epsilon = trial.suggest_uniform(\"epsilon\", 0.01, 0.1)\n",
            "<ipython-input-54-cd7b9cca50e1>:45: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr_opt = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
            "<ipython-input-54-cd7b9cca50e1>:46: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  gamma = trial.suggest_uniform(\"gamma\", 0.8, 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 100 - Média últimos 100: -363.940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:51:45,228] Trial 0 finished with value: -313.85 and parameters: {'epsilon': 0.014544694684307403, 'lr': 0.05889262823650827, 'gamma': 0.8990429177788452}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -313.850\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -434.240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:51:50,789] Trial 1 finished with value: -494.22 and parameters: {'epsilon': 0.07141525964359295, 'lr': 0.0009940448922967178, 'gamma': 0.9188266121349343}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -494.220\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -403.850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:51:58,523] Trial 2 finished with value: -429.22 and parameters: {'epsilon': 0.019805470832897288, 'lr': 0.0007777855488462078, 'gamma': 0.8926711003270273}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -429.220\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -438.380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:04,245] Trial 3 finished with value: -445.79 and parameters: {'epsilon': 0.09918132024840881, 'lr': 0.002801921585642179, 'gamma': 0.8796824584171996}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -445.790\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -465.050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:09,483] Trial 4 finished with value: -415.37 and parameters: {'epsilon': 0.010244836950964564, 'lr': 1.5426422287519976e-05, 'gamma': 0.9434083463650674}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -415.370\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -403.170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:14,842] Trial 5 finished with value: -480.44 and parameters: {'epsilon': 0.07873344867744825, 'lr': 0.00029280001356382326, 'gamma': 0.9181220614031725}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -480.440\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -411.950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:20,149] Trial 6 finished with value: -563.6 and parameters: {'epsilon': 0.05727481932995039, 'lr': 7.474057627118825e-05, 'gamma': 0.9814191019328273}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -563.600\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -489.800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:25,231] Trial 7 finished with value: -473.6 and parameters: {'epsilon': 0.09858625085592157, 'lr': 0.010140053306812906, 'gamma': 0.9508975101378554}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -473.600\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -403.460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:30,598] Trial 8 finished with value: -409.51 and parameters: {'epsilon': 0.07271053573891635, 'lr': 0.0020323300156581472, 'gamma': 0.8520360954441575}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -409.510\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -417.770\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-21 01:52:35,339] Trial 9 finished with value: -384.77 and parameters: {'epsilon': 0.04397601592222559, 'lr': 2.3949912698919786e-05, 'gamma': 0.8140705070265737}. Best is trial 0 with value: -313.85.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Híbrido] Episódio 200 - Média últimos 100: -384.770\n",
            "Optuna: Melhores parâmetros para Híbrido em Taxi-v3: {'epsilon': 0.014544694684307403, 'lr': 0.05889262823650827, 'gamma': 0.8990429177788452}\n",
            "Executando Híbrido em Taxi-v3 com parâmetros: {'epsilon': 0.014544694684307403, 'lr': 0.05889262823650827, 'gamma': 0.8990429177788452}\n",
            "[Híbrido] Episódio 100 - Média últimos 100: -361.120\n",
            "[Híbrido] Episódio 200 - Média últimos 100: -327.590\n",
            "[Híbrido] Episódio 300 - Média últimos 100: -269.880\n",
            "[Híbrido] Episódio 400 - Média últimos 100: -249.340\n",
            "[Híbrido] Episódio 500 - Média últimos 100: -217.420\n",
            "[Híbrido] Episódio 600 - Média últimos 100: -218.120\n",
            "[Híbrido] Episódio 700 - Média últimos 100: -207.810\n",
            "[Híbrido] Episódio 800 - Média últimos 100: -202.830\n",
            "[Híbrido] Episódio 900 - Média últimos 100: -198.830\n",
            "[Híbrido] Episódio 1000 - Média últimos 100: -205.980\n",
            "[Híbrido] Episódio 1100 - Média últimos 100: -197.360\n",
            "[Híbrido] Episódio 1200 - Média últimos 100: -201.120\n",
            "[Híbrido] Episódio 1300 - Média últimos 100: -192.070\n",
            "[Híbrido] Episódio 1400 - Média últimos 100: -195.340\n",
            "[Híbrido] Episódio 1500 - Média últimos 100: -202.560\n",
            "[Híbrido] Episódio 1600 - Média últimos 100: -203.460\n",
            "[Híbrido] Episódio 1700 - Média últimos 100: -200.030\n",
            "[Híbrido] Episódio 1800 - Média últimos 100: -203.190\n",
            "[Híbrido] Episódio 1900 - Média últimos 100: -196.350\n",
            "[Híbrido] Episódio 2000 - Média últimos 100: -203.450\n",
            "[Híbrido] Episódio 2100 - Média últimos 100: -190.410\n",
            "[Híbrido] Episódio 2200 - Média últimos 100: -186.650\n",
            "[Híbrido] Episódio 2300 - Média últimos 100: -197.350\n",
            "[Híbrido] Episódio 2400 - Média últimos 100: -193.190\n",
            "[Híbrido] Episódio 2500 - Média últimos 100: -199.550\n",
            "[Híbrido] Episódio 2600 - Média últimos 100: -195.630\n",
            "[Híbrido] Episódio 2700 - Média últimos 100: -188.350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Experimentos com Learning Rates Fixos e Plotagem\n",
        "\n",
        "# Para cada ambiente e para cada learning rate fixo, executa os três algoritmos\n",
        "# (usando gamma=0.95 e epsilon=0.1) e plota as curvas de aprendizado.\n",
        "for env_name, config in env_configs.items():\n",
        "    for lr in learning_rates:\n",
        "        fig, ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "        # Expected-SARSA com learning rate fixo\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            env = gym.make(env_name, max_episode_steps=500)\n",
        "        else:\n",
        "            env = gym.make(env_name)\n",
        "        print(f\"Executando Expected-SARSA em {env_name} com lr fixo = {lr}\")\n",
        "        rewards_ES, _ = run_expected_sarsa(env, episodes=config[\"episodes\"], lr=lr, gamma=0.95, epsilon=0.1)\n",
        "        env.close()\n",
        "        plot_learning_curve(ax, rewards_ES, window=50, label=\"Expected-SARSA\")\n",
        "\n",
        "        # Monte Carlo com learning rate fixo\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            env = gym.make(env_name, max_episode_steps=500)\n",
        "        else:\n",
        "            env = gym.make(env_name)\n",
        "        print(f\"Executando Monte Carlo em {env_name} com lr fixo = {lr}\")\n",
        "        rewards_MC, _ = run_montecarlo2(env, episodes=config[\"episodes\"], lr=lr, gamma=0.95, epsilon=0.1)\n",
        "        env.close()\n",
        "        plot_learning_curve(ax, rewards_MC, window=50, label=\"Monte Carlo\")\n",
        "\n",
        "        # Híbrido com learning rate fixo\n",
        "        if env_name == \"CliffWalking-v0\":\n",
        "            env = gym.make(env_name, max_episode_steps=500)\n",
        "        else:\n",
        "            env = gym.make(env_name)\n",
        "        print(f\"Executando Híbrido em {env_name} com lr fixo = {lr}\")\n",
        "        rewards_HYB, _ = run_hybrid_expected_sarsa_mc(env, episodes=config[\"episodes\"], lr=lr, gamma=0.95, epsilon=0.1)\n",
        "        env.close()\n",
        "        plot_learning_curve(ax, rewards_HYB, window=50, label=\"Híbrido\")\n",
        "\n",
        "        ax.set_title(f\"{env_name} - Fixed Learning Rate: {lr}\")\n",
        "        ax.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "EswJPm_zJJgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}